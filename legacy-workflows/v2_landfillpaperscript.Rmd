---
title: "Landfill paper script version 2"
output:
  html_document:
    df_print: paged
---
Tutorials and papers used: 
Callahan et al., 2016
Handley et al., "2018 ASM Using R to Analyze the Bacterial Microbiome Workshop"
https://benjjneb.github.io/dada2/tutorial_1_8.html
https://astrobiomike.github.io/
http://joey711.github.io/phyloseq-demo/phyloseq-demo.html
Full_16S_workflow_DRMD_21071
https://mibwurrepo.github.io/Microbial-bioinformatics-introductory-course-Material-2018/citation.html


Further reading: 

http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html

Links to other methods to detect differentially abundant taxa:
Random Forest in R example: https://rpubs.com/michberr/randomforestmicrobe
LEfSe: https://bitbucket.org/biobakery/biobakery/wiki/lefse
ANCOM: https://www.ncbi.nlm.nih.gov/pubmed/26028277


Load the required packages 
```{r}
#install bioconductor if needed
#if (!requireNamespace("BiocManager", quietly = TRUE))
    #install.packages("BiocManager")
#BiocManager::install(version = "3.13")

require("tidyverse")
library("dplyr")
library("rmarkdown")
library("vegan")
library("PoiClaClu")
library("doParallel")
library("plotly")
#BiocManager::install("microbiome")
library("microbiome"); packageVersion("microbiome")
#BiocManager::install("DESeq2")
library("DESeq2")
library("ggpubr")
#library("data.table")
library("viridis")
#library(DECIPHER); package.version("DECIPHER")
library("plyr"); packageVersion("plyr")
library("magrittr")
library("scales")
#library(grid)
#library(reshape2)
#library(randomForest)

#following code chunk doesn't work anymore, i'll just re-load/re-install the packages if i need them later. 
#not even sure I need all of these packages as of now

#options(width = 98) 
#opts_chunk$set(message = FALSE, error = FALSE, warning = FALSE,
               #cache = TRUE, fig.width = 8, fig.height = 7)
#.cran_packages <- c("ggplot2", "gridExtra")
#.bioc_packages <- c("dada2", "phyloseq", "DECIPHER", "phangorn")
#.inst <- .cran_packages %in% installed.packages()
#if(any(!.inst)) {
 #  install.packages(.cran_packages[!.inst])}
#.inst <- .bioc_packages %in% installed.packages()
#if(any(!.inst)) {
   #source("http://bioconductor.org/biocLite.R")
  # biocLite(.bioc_packages[!.inst], ask = F)}
#Error: With R version 3.5 or greater, install Bioconductor packages using BiocManager; see https://bioconductor.org/install

# Load packages into session, and print package version
#sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)
#set.seed(100)

```


#         PART I: [Input is Paired end reads, output is a phyloseq object]
*Step 1: Getting the files ready
*Step 2: Quality checking 
  Step 2.1: Filtering
  Step 2.2: Checking quality after filtering 
*Step 3: Dereplication step
*Step 4: Error rates
*STEP 5: CONSTRUCT ASVS WITH DADA2 
*Step 6: Make a sequence table and remove chimeras
*Step 7: Assign taxonomy, using the chimera-free reads
  Step 7.1: Assign taxonomy another way, fits with the variance stabilizing transformation in step 10 
*Step 8: Make a tree
*Step 9:  Read in in your metadata table
*Step 10: Variance stabilizing transformation; ignore if not transforming data
*Step 11: Make a phyloseq object on transformed data
  Step 11.1: Make a phyloseq object w/o transformed data
  
  Important files made in part one:
  all= phyloseq object (w/o) variance transformation, all samples full time of experiment (no NL1-3)
  vst_all = phyloseq object WITH variance transformation, all samples full time of experiment (no NL1-3)
  
  
----

#STEP1: GETTING FILES READY
Live samples: Create a path to the files being used
```{r}
AB_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/PEAR merged files AB")
Fe_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/PEAR merged files Fe")
S_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/PEAR merged files S")
NL_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/PEAR merged files NL")
all_live_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/all_live")

#path to a new file where I'll put the filtered 
AB_filt_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/AB_filt2")
Fe_filt_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/Fe_filt")
S_filt_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/S_filt")
NL_filt_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/NL_filt")
all_filt_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/all_live_filt")


```

#STEP2: QUALITY CHECKING
Live Samples: Check quality of paired reads 
```{r, eval=FALSE}

#sort ensures forward/reverse reads are in the same order
#except I already have paired so this is pretty much just sorting them in by name in general? I could've skipped this step probably
fns_AB = sort(list.files(AB_path, full.names = TRUE)) #done 
fns_Fe = sort(list.files(Fe_path, full.names = TRUE)) 
fns_S = sort(list.files(S_path, full.names = TRUE))
fns_NL = sort(list.files(NL_path, full.names = TRUE))
fns_all= sort(list.files(all_live_path, full.names = TRUE))

#Check quality of the merged reads 
ABi <- sample(length(fns_AB), 17) 
for(i in ABi) { print(plotQualityProfile(fns_AB[i]) + ggtitle("AB paired")) }

NLi <- sample(length(fns_NL), 30) 
for(i in NLi) { print(plotQualityProfile(fns_NL[i]) + ggtitle("NL paired")) }

Fei <- sample(length(fns_Fe), 18) 
for(i in Fei) { print(plotQualityProfile(fns_Fe[i]) + ggtitle("Fe paired")) }

Si <- sample(length(fns_S), 18) 
for(i in Si) { print(plotQualityProfile(fns_S[i]) + ggtitle("S paired")) }

alli <- sample(length(fns_all), 18) 
for(i in Si) { print(plotQualityProfile(fns_S[i]) + ggtitle("S paired")) }

```

#STEP 2.1: FILTERING
We combine these trimming parameters with standard filtering parameters,
the most important being the enforcement of a maximum of 2 expected
errors per-read [@edgar2015unoise].[From Callahan et al.]
Also trimmed first and last 10 bp because these are likely to be errors [from callahan et al.,]
Live Samples Filer and Trim
```{r, eval=FALSE}
#AB
filt_AB <- file.path(AB_filt_path, basename(fns_AB))
for(i in seq_along(fns_AB)) {
  filterAndTrim(fns_AB, 
                filt_AB, 
                rev = NULL, 
                filt.rev =NULL, 
                trimLeft=c(10), truncLen=c(290),
                maxN=0, maxEE=c(2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)}
#Fe
if(!file_test("-d", Fe_filt_path)) dir.create(Fe_filt_path)
filt_Fe <- file.path(Fe_filt_path, basename(fns_Fe))
for(i in seq_along(fns_Fe)) {
  filterAndTrim(fns_Fe, 
                filt_Fe, 
                rev = NULL, 
                filt.rev =NULL, 
                trimLeft=c(10), truncLen=c(290),
                maxN=0, maxEE=c(2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)}
#S
if(!file_test("-d", S_filt_path)) dir.create(S_filt_path)
filt_S <- file.path(S_filt_path, basename(fns_S))
for(i in seq_along(fns_S)) {
  filterAndTrim(fns_S, 
                filt_S, 
                rev = NULL, 
                filt.rev =NULL, 
                trimLeft=c(10), truncLen=c(290),
                maxN=0, maxEE=c(2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)}
#NL
if(!file_test("-d", NL_filt_path)) dir.create(NL_filt_path)
filt_NL <- file.path(NL_filt_path, basename(fns_NL))
for(i in seq_along(fns_NL)) {
  filterAndTrim(fns_NL, filt_NL, 
                rev = NULL, 
                filt.rev =NULL, 
                trimLeft=c(10), truncLen=c(290),
                maxN=0, maxEE=c(2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)}


#all_live 
if(!file_test("-d", all_live_path)) dir.create(all_filt_path)
filt_all <- file.path(all_filt_path, basename(fns_all))
for(i in seq_along(fns_all)) {filterAndTrim(fns_all, filt_all, 
                rev = NULL, 
                filt.rev =NULL, 
                trimLeft=c(10), truncLen=c(290),
                maxN=0, maxEE=c(2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)}

```

#STEP 2.2: QUALITY CHECK AFTER FILTERING
Live Samples: Check trimmed quality 
```{r, eval=FALSE}
AB_filt = sample(length(filt_AB), 17)
for(i in AB_filt) {print(plotQualityProfile(filt_AB[i])+ ggtitle("AB filt"))}

NL_filt = sample(length(filt_NL), 30)
for(i in NL_filt) {print(plotQualityProfile(filt_NL[i])+ ggtitle("NL filt"))}

Fe_filt_i <- sample(length(filt_Fe), 18) 
for(i in Fe_filt_i) {print(plotQualityProfile(filt_Fe[i]) + ggtitle("Fe filt paired")) }

S_filt = sample(length(filt_S), 18)
for(i in S_filt) {print(plotQualityProfile(filt_S[i])+ ggtitle("S filt"))}

all_filt <- sample(length(filt_all), 83) 
for(i in all_filt) { print(plotQualityProfile(filt_all[i]) + ggtitle("All live filt"))}


```

#STEP 3: DEPEPLICATION 
Live Samples: Dereplication
```{r, eval=FALSE}
derep_AB = derepFastq(filt_AB)
sam.names <- sapply(strsplit(basename(filt_AB), "_"), `[`, 1)
names(derep_AB) <- sam.names

derep_Fe = derepFastq(filt_Fe)
sam.names_Fe <- sapply(strsplit(basename(filt_Fe), "_"), `[`, 1)
names(derep_Fe) <- sam.names_Fe

derep_S = derepFastq(filt_S)
sam.names_S <- sapply(strsplit(basename(filt_S), "_"), `[`, 1)
names(derep_S) <- sam.names_S

derep_NL = derepFastq(filt_NL)
sam.names_NL<- sapply(strsplit(basename(filt_NL), "_"), `[`, 1)
names(derep_NL) <- sam.names_NL

derep_all = derepFastq(filt_all)
sam.names_all <- sapply(strsplit(basename(filt_all), "_"), `[`, 1)
names(derep_all) <- sam.names_all

```

#STEP 4: ERROR RATES
For learning errors, used code here because the one in the Callahan et al. paper didn't work, but this is by the same author:
https://benjjneb.github.io/dada2/tutorial_1_8.html
might need to change the "nbases" parameter to see if the fit improves
Live samples: Learning Errors 
```{r, eval=FALSE}
err_AB = learnErrors(derep_AB, multithread = TRUE) 
err_Fe = learnErrors(derep_Fe, multithread = TRUE)
err_S = learnErrors(derep_Fe, multithread = TRUE)
err_NL = learnErrors(derep_Fe, multithread = TRUE)
error_all = learnErrors(derep_all, multithread = TRUE)

plotErrors(err_AB, nominalQ = TRUE) 
plotErrors(err_NL, nominalQ = TRUE)
plotErrors(err_Fe, nominalQ = TRUE)
plotErrors(err_S, nominalQ = TRUE)

plotErrors(error_all, nominalQ=TRUE) #it looks like this is an okay fit, but I'll try increasing the number of bases

err_all = learnErrors(derep_all, multithread = TRUE, nbases = 1e9) #this used all of the samples, we'll see how long it takes
plotErrors(err_all, nominalQ=TRUE)


```

#STEP 5: CONSTRUCT ASVS WITH DADA2 
Live samples: DADA2 sequence inference method
```{r, eval=FALSE}
dada_AB <- dada(derep_AB, err=err_AB, pool=TRUE, multithread = TRUE)
dada_Fe <- dada(derep_Fe, err=err_Fe, pool=TRUE, multithread = TRUE)
dada_S <- dada(derep_S, err=err_S, pool=TRUE, multithread = TRUE)
dada_NL <- dada(derep_NL, err=err_NL, pool=TRUE, multithread = TRUE)
dada_all = dada(derep_all, err=err_all, pool=TRUE, multithread= TRUE)
```

#STEP 6: SEQ TABLE + REMOVE CHIMERAS
Live samples:Construct Sequence table + Remove Chimeras 
```{r, eval=FALSE}

seqtab.AB = makeSequenceTable(dada_AB)
seqtab.Fe = makeSequenceTable(dada_Fe)
seqtab.S = makeSequenceTable(dada_S)
seqtab.NL = makeSequenceTable(dada_NL)
seqtab = makeSequenceTable(dada_all)


nochim_seqtab.AB <- removeBimeraDenovo(seqtab.AB, method="consensus", multithread=TRUE, verbose=TRUE)
nochim_seqtab.Fe <- removeBimeraDenovo(seqtab.Fe, method="consensus", multithread=TRUE, verbose=TRUE)
nochim_seqtab.S <- removeBimeraDenovo(seqtab.S, method="consensus", multithread=TRUE, verbose=TRUE)
nochim_seqtab.NL <- removeBimeraDenovo(seqtab.NL, method="consensus", multithread=TRUE, verbose=TRUE)

nochim_seqtab.all <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)

```

#STEP 7: CLASSIFY TAXONOMY
Live samples: Classify Taxonomy
```{r, eval=FALSE}

ref_fasta = "~/Desktop/Landfill sequencing data processing /silva_nr_v138_train_set.fa.gz"
print(ref_fasta)

taxtab.AB = assignTaxonomy(nochim_seqtab.AB, refFasta = ref_fasta)
colnames(taxtab.AB) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")

taxtab.Fe = assignTaxonomy(nochim_seqtab.Fe, refFasta = ref_fasta)
colnames(taxtab.Fe) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")

taxtab.S = assignTaxonomy(nochim_seqtab.S, refFasta = ref_fasta)
colnames(taxtab.S) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")

taxtab.NL = assignTaxonomy(nochim_seqtab.NL, refFasta = ref_fasta)
colnames(taxtab.NL) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")

taxtab.all = assignTaxonomy(nochim_seqtab.all, refFasta = ref_fasta)
colnames(taxtab.all) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")

```

#STEP 7.1: ASSIGN TAXONOMY ANOTHER WAY. 
```{r, eval=FALSE}

taxa <- assignTaxonomy(nochim_seqtab.all, "~/Desktop/Landfill sequencing data processing /silva_nr_v138_train_set.fa.gz", multithread=TRUE)

#Give our seq headers more manageable names (ASV_1, ASV_2...) and to write some tables. 
asv_seqs <- colnames(nochim_seqtab.all)
asv_headers <- vector(dim(nochim_seqtab.all)[2], mode="character")

for (i in 1:dim(nochim_seqtab.all)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")

# count table:
asv_tab <- t(nochim_seqtab.all)  
# HEY. This looks like MT flipped the seqtab.nochim table for some reason. I'll leave it here for now and fix it below...
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

# creating table of taxonomy [and setting any that are unclassified as "NA"....note that I didn't need to do this part, but left the script in, here, just as comments]. 
ranks <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")
colnames(taxa) <- ranks
rownames(taxa) <- gsub(pattern=">", replacement="", x=asv_headers)
write.table(taxa, "ASVs_taxonomy.tsv", sep = "\t", quote=F, col.names=NA)

# continuing on....
# count_tab will hold the ASV and sample names
count_tab <- read.table("ASVs_counts.tsv", header=T, row.names=1,
                        check.names=F, sep="\t")
# tax_tab will hold the ASVs and the taxa that go with them.
tax_tab <- as.matrix(read.table("ASVs_taxonomy.tsv", header=T,
                                row.names=1, check.names=F, sep="\t"))
#check to see if these look reasonable. Note, if you have a lot of data, use 'head'
count_tab
tax_tab
#At this stage, 'count_tab' has rows that are 'ASV_1' (for example), and columns that are 'name.fastq' and the data are counts.
#At this stage, 'tax_tab' has rows that are 'ASV_1' (for example), and columns that are taxonomic classes and the data are the taxonomic lineages of each ASV. 

# If your count_tab does NOT have the sample names in the rows, then you need to fix that. The ASV data tables and the #metadata tables MUST be formatted so that the sample names are in rows (or more, they must both be oriented the same way #and the statistical people prefer samples in rows). I guess the tax_tab should have the ASVs in the same orientation as the #count_tab. If you need to fix them...

#this script will do so. BUT, you should rename the original files so that you preserve them. I've done that by adding a 't' here, which is the command for transposing a table (so you'll know by the name that it's a transposed table).
count_tab_t <- t(count_tab)
tax_tab_t <- t(tax_tab)

#check them again to make sure it's right.
count_tab_t
tax_tab_t
#They should now both have the sample name.fastq on the rows in these version of the tables.

#It would be smart here to check the row names of your count_tab to make sure they're in the same order as your prepared metadata table. Cuz now's a good time to fix it if it's not!
rownames(count_tab_t)

```

REPEAT STEPS 2-7 FOR THE KILLED MICROCOSMS
```{r, Killed_quality check,  eval=FALSE}
NKi <- sample(length(fns_NK), 33) 
for(i in NKi) { print(plotQualityProfile(fns_NK[i]) + ggtitle("NK paired")) }


killedi = sample(length(fns_killed), 94)
for(i in killedi) {print (plotQualityProfile(fns_killed[i])+ ggtitle("killed paired"))}


```

```{r, STEP2.1, eval=FALSE}
#killed 
killed_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/killed")

#filt path
killed_filt_path = file.path("~/Desktop/Landfill sequencing data processing /PEAR_merged/killed_filt")
fns_killed = sort(list.files(killed_path, full.names = TRUE))


#filter and trim
#all killed 
if(!file_test("-d", killed_path)) dir.create(killed_filt_path)
filt_killed <- file.path(killed_filt_path, basename(fns_killed))
for(i in seq_along(fns_killed)) {filterAndTrim(fns_killed, filt_killed, 
                rev = NULL, 
                filt.rev =NULL, 
                trimLeft=c(10), truncLen=c(290),
                maxN=0, maxEE=c(2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)}
#many failed quality check so I'm going to filter and trim the experiments separately 
#I didn't finish running this.. but it looks like many worked
#actually I'm just going to re-do this one again and let it run all the way through 


#check quality 
killed_filt = sample(length(filt_killed), 93) 

killed_filt = sample(length(filt_killed), 93) 
for(i in killed_filt) {print(plotQualityProfile(filt_killed[i]) + ggtitle("killed filt paired")) }


#ABK1-T6 has low read count: 3290
#ABK3-T1 has low read count: 1484 
#SK1-T6 is poor quality and low read count, definetly out
#SK3-T4 is low quality
#Some FeKs have read counts on the lower end of the spectrum; FeK3-T7 
#Lots of NKs have low quality and or/ low read count: NK6-T7, NK2-T2 
#ABK1-T6 has low read count: 3290
#ABK3-T1 has low read count: 1484 
#SK1-T6 is poor quality and low read count, definetly out
#SK3-T4 is low quality
#Some FeKs have read counts on the lower end of the spectrum; FeK3-T7 
```

```{r, step3-4, eval=FALSE}
#dereplication
derep_killed = derepFastq(filt_killed)
sam.names_killed = sapply(strsplit(basename(filt_killed), "_"), `[`, 1)
names(derep_killed) = sam.names_killed

#learning errors
err_killed = learnErrors(derep_killed, multithread = TRUE, nbases = 1e9) #this used all of the samples, we'll see how long it takes.q§
plotErrors(err_killed, nominalQ=TRUE)

```

```{r, steps5-7, eval=FALSE}
#dada seq inference method
dada_killed = dada(derep_killed, err=err_killed, pool=TRUE, multithread= TRUE)


#construct seq tab and remove chimeras 
seqtab_killed = makeSequenceTable(dada_killed)
nochim_seqtab_killed = removeBimeraDenovo(seqtab_killed, method="consensus", multithread=TRUE, verbose=TRUE)

#classify taxanomy
ref_fasta = "~/Desktop/Landfill sequencing data processing /silva_nr_v138_train_set.fa.gz"
taxtab_killed = assignTaxonomy(nochim_seqtab_killed, refFasta = ref_fasta)
colnames(taxtab_killed) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")


```


#STEP 8: MAKE A TREE 
Live: Construct phylogenetic trees
```{r, eval=FALSE}

# Tree for all samples
all_seq = getSequences(nochim_seqtab.all)
names(all_seq) = all_seq
alignment_all = AlignSeqs(DNAStringSet(all_seq), anchor=NA) #multiple seq alignment
all_phang.align<- phyDat(as(alignment_all, "matrix"), type="DNA") #The phangorn R package is then used to construct a phylogenetic tree.
#neighborjoining tree
all_dm <- dist.ml(all_phang.align) 
all_treeNJ <- NJ(all_dm) # Note, tip order != sequence order
all_fit = pml(all_treeNJ, data=all_phang.align)
#GTR+G+I tree
all_fitGTR <- update(all_fit, k=4, inv=0.2)
all_fitGTR <- optim.pml(all_fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                    rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)



#seperate trees for each experimental group

#AB
seqs_AB <- getSequences(nochim_seqtab.AB)
names(seqs_AB) <- seqs_AB # This propagates to the tip labels of the tree
alignment_AB <- AlignSeqs(DNAStringSet(seqs_AB), anchor=NA)#Multiple sequence alightment

#The phangorn R package is then used to construct a phylogenetic tree. 
#Here we first construct a neighbor-joining tree, and then fit a GTR+G+I (Generalized time-reversible with Gamma rate variation) maximum likelihood tree using the neighbor-joining tree as a starting point.

phang.align <- phyDat(as(alignment_AB, "matrix"), type="DNA")
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phang.align)
fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                    rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)


#NL 
#NL tree construction
seqs_NL <- getSequences(nochim_seqtab.NL)
names(seqs_NL) <- seqs_NL # This propagates to the tip labels of the tree
alignment_NL <- AlignSeqs(DNAStringSet(seqs_NL), anchor=NA)
phang.align <- phyDat(as(alignment_NL, "matrix"), type="DNA")
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phang.align)

## negative edges length changed to 0!
fitGTR_NL <- update(fit, k=4, inv=0.2)
fitGTR_NL <- optim.pml(fitGTR_NL, model="GTR", optInv=TRUE, optGamma=TRUE,
                    rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)

```

STEP 8: MAKE A TREE
Killed: Construct phylogenetic trees
```{r, eval=FALSE}

killed_seq = getSequences(nochim_seqtab_killed)
names(killed_seq) = killed_seq
alignment_killed = AlignSeqs(DNAStringSet(killed_seq), anchor=NA) #multiple seq alignment
killed_phang.align = phyDat(as(alignment_killed, "matrix"), type="DNA") #The phangorn R package is then used to construct a phylogenetic tree.
#neighborjoining tree
killed_dm <- dist.ml(killed_phang.align) #compute pairwise distances 
killed_treeNJ <- NJ(killed_dm) # Note, tip order != sequence order
killed_fit = pml(killed_treeNJ, data=killed_phang.align)  # computes likelihood of a tree given a sequence alignment and a model
#GTR+G+I tree
killed_fitGTR <- update(killed_fit, k=4, inv=0.2) # I'm not sure why k and inv are set at these values
killed_fitGTR <- optim.pml(killed_fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                    rearrangement = "stochastic", control = pml.control(trace = 0))



```

#Step 9: Load metadata
Load metadata + make sure metadata files match the seq tab files
```{r, eval=FALSE}
AB_metadata = read.csv("~/Desktop/Landfill sequencing data processing /PEAR_merged/AB_metadata.csv")
Fe_metadata = read.csv("~/Desktop/Landfill sequencing data processing /PEAR_merged/Fe_metadata.csv")
S_metadata = read.csv("~/Desktop/Landfill sequencing data processing /PEAR_merged/S_metadata.csv")
NL_metadata = read.csv("~/Desktop/Landfill sequencing data processing /PEAR_merged/NL_metadata.csv")
all_metadata = read.csv("~/Desktop/Landfill sequencing data processing /PEAR_merged/allmetadata.csv")
killed_metadata = read.csv("~/Desktop/Landfill sequencing data processing /PEAR_merged/killed_metadata.csv")

#all 
all(rownames(nochim_seqtab.all) %in% all_metadata$sample)
rownames(all_metadata) = all_metadata$sample
rownames(nochim_seqtab.all) = c("AB1","AB1.1","AB1.2","AB1.3","AB1.4","AB1.5", "AB2",   "AB2.1", "AB2.2", "AB2.3", "AB2.4" ,"AB2.5", "AB3"  , "AB3.1", "AB3.2", "AB3.3" ,"AB3.4","Fe1", "Fe1.1", "Fe1.2", "Fe1.3", "Fe1.4", "Fe1.5", "Fe2", "Fe2.1", "Fe2.2", "fe2", "Fe2.3", "Fe2.4", "Fe3", "Fe3.1", "Fe3.2", "Fe3.3", "Fe3.4", "Fe3.5", "NL1", "NL1.1", "NL1.2", "NL1.3", "NL2", "NL2.1", "NL2.2", "NL2.3", "NL3", "NL3.1", "NL3.2", "NL3.3", "NL4", "NL4.1", "NL4.2", "NL4.3", "NL4.4", "NL4.5", "NL5", "NL5.1", "NL5.2", "NL5.3", "NL5.4", "NL5.5", "NL6", "NL6.1", "NL6.2", "NL6.3", "NL6.4", "NL6.5","S1", "S1.1", "S1.2", "S1.3", "S1.4", "S1.5", "S2", "S2.1", "S2.2", "S2.3", "S2.4", "S2.5", "S3", "S3.1", "S3.2", "S3.3", "S3.4", "S3.5")
identical(rownames(nochim_seqtab.all), rownames(all_metadata)) 
rownames(nochim_seqtab.all)
rownames(all_metadata)

#I think my metadata should be factors instead of characters?
#I'm going to try to update my metadata file and make a new ps object 
#all_metadata$Spike = as.factor(all_metadata$Spike)
#all_metadata$Sample_Time = as.factor(all_metadata$Sample_Time)

#killed 
all(rownames(nochim_seqtab_killed) %in% killed_metadata$sample)
rownames(killed_metadata) = killed_metadata$sample
rownames(nochim_seqtab_killed) =c("ABK1",   "ABK1.1", "ABK1.2", "ABK1.3" ,"ABK1.4", "ABK1.5", "ABK1.6" ,"ABK2" ,  "ABK2.1", "ABK2.2", "ABK2.3" ,"ABK2.4" ,"ABK2.5", "ABK2.6", "ABK3",   "ABK3.1", "ABK3.2", "ABK3.3", "ABK3.4" ,"ABK3.5", "ABK3.6", "FeK1"  , "FeK1.1", "FeK1.2", "FeK1.3", "FeK1.4" ,"FeK1.5", "FeK1.6", "FeK2" ,  "FeK2.1", "FeK2.2" ,"FeK2.3" ,"feK2" ,  "FeK2.4" ,"FeK2.5", "FeK3", "FeK3.1", "FeK3.2" ,"FeK3.3", "FeK3.4" , "FeK3.5" ,"FeK3.6" ,"NK1"  ,  "NK1.1" , "NK1.2" , "NK1.3",  "NK2" ,   "NK2.1" , "NK2.2" , "NK2.3"  , "NK3",    "NK3.1" , "NK3.2" , "NK3.3" , "NK4"   , "NK4.1" , "NK4.2" , "NK4.3" , "NK4.4" , "NK4.5"  ,"NK4.6",  "NK5"  ,  "NK5.1" , "NK5.2" , "NK5.3" , "NK5.4" , "NK5.5",  "NK5.6",  "NK6",    "NK6.1" ,"NK6.2",  "NK6.3" , "NK6.4",  "NK6.5" , "NK6.6" , "SK1" ,   "SK1.1",  "SK1.2",  "SK1.3" , "SK2",  "SK2.1" , "SK2.2" , "SK2.3" , "SK2.4" , "SK2.5" , "SK2.6",  "SK3" ,   "SK3.1",  "SK3.2" ,"SK3.3","SK3.4" , "SK3.5" , "SK3.6")
identical(rownames(nochim_seqtab_killed), rownames(killed_metadata)) 



#AB
#row names in nochim seq tab and in the metadata file need to be identitical
all(rownames(seqtab.AB) %in% AB_metadata$sample)
#this needs to be true in order to continue, the sample names in the metadata table need to be the same as in the seq table 
rownames(AB_metadata) = AB_metadata$sample #this added a row name that was the same as the seqtab row name
rownames(nochim_seqtab.AB) = c("AB1","AB1.1","AB1.2","AB1.3","AB1.4","AB1.5", "AB2",   "AB2.1", "AB2.2", "AB2.3", "AB2.4" ,"AB2.5", "AB3"  , "AB3.1", "AB3.2", "AB3.3" ,"AB3.4")
rownames(nochim_seqtab.AB)
identical(rownames(nochim_seqtab.AB), rownames(AB_metadata)) 

#NL
all(rownames(seqtab.NL) %in% NL_metadata$sample)
#this needs to be true in order to continue, the sample names in the metadata table need to be the same as in the seq table 
rownames(NL_metadata) = NL_metadata$sample #this added a row name that was the same as the seqtab row name
print(NL_metadata$sample)
rownames(nochim_seqtab.NL) =  c("NL1", "NL1.1", "NL1.2", "NL1.3", "NL2", "NL2.1", "NL2.2", "NL2.3", "NL3", "NL3.1", "NL3.2", "NL3.3", "NL4", "NL4.1", "NL4.2", "NL4.3", "NL4.4", "NL4.5", "NL5", "NL5.1", "NL5.2", "NL5.3", "NL5.4", "NL5.5", "NL6", "NL6.1", "NL6.2", "NL6.3", "NL6.4", "NL6.5")
#this step is because otherwise rownames appear like this: 
##> rownames(nochim_seqtab.NL)
##[1] "NL1" "NL1" "NL1" "NL1" "NL2" "NL2" "NL2" "NL2" "NL3" "NL3" "NL3" "NL3" "NL4" "NL4" "NL4" "NL4"
##[17] "NL4" "NL4" "NL5" "NL5" "NL5" "NL5" "NL5" "NL5" "NL6" "NL6" "NL6" "NL6" "NL6" "NL6"
rownames(nochim_seqtab.NL)
rownames(NL_metadata)
identical(rownames(nochim_seqtab.NL), rownames(NL_metadata)) 


#Fe
all(rownames(seqtab.Fe) %in% Fe_metadata$sample)
#this needs to be true in order to continue, the sample names in the metadata table need to be the same as in the seq table 
##FALSE 
#Fe is fudged up with the row names, so I have to change them in the metadata file to match seqtab.Fe

rownames(Fe_metadata) = Fe_metadata$sample #this added a row name that was the same as the seqtab row name
print(Fe_metadata$sample)
rownames(nochim_seqtab.Fe)
rownames(nochim_seqtab.Fe) =  c ("Fe1", "Fe1.1", "Fe1.2", "Fe1.3", "Fe1.4", "Fe1.5", "Fe2", "Fe2.1", "Fe2.2", "fe2", "Fe2.3", "Fe2.4", "Fe3", "Fe3.1", "Fe3.2", "Fe3.3", "Fe3.4", "Fe3.5")
rownames(nochim_seqtab.Fe)
rownames(Fe_metadata)
identical(rownames(nochim_seqtab.Fe), rownames(Fe_metadata)) 



#S
all(rownames(seqtab.S) %in% S_metadata$sample)
#this needs to be true in order to continue, the sample names in the metadata table need to be the same as in the seq table 
rownames(S_metadata) = S_metadata$sample #this added a row name that was the same as the seqtab row name
print(S_metadata$sample)
rownames(nochim_seqtab.S)
rownames(nochim_seqtab.S) =  c ("S1", "S1.1", "S1.2", "S1.3", "S1.4", "S1.5", "S2", "S2.1", "S2.2", "S2.3", "S2.4", "S2.5", "S3", "S3.1", "S3.2", "S3.3", "S3.4", "S3.5")
rownames(nochim_seqtab.S)
rownames(S_metadata)
identical(rownames(nochim_seqtab.S), rownames(S_metadata)) 



```

#Step 10: Variance stablilizing transformation
```{r, eval=FALSE}
#make a count table 
asv_tab <- t(nochim_seqtab.all)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

count_tab = read.table("ASVs_counts.tsv", header=T, row.names=1,
                        check.names=F, sep="\t")


#variance stabilizing transformation

design = as.formula(~spike)
modelMartrix = model.matrix(design, data = all_metadata)
all_metadata$sample = factor(all_metadata$sample, 
                             levels = c("Antibiotics", "Fe(OH)3", "Control", "Na2SO4"))
modelMatrix = model.matrix(design, data= all_metadata)


deseq_counts <- DESeqDataSetFromMatrix(count_tab, colData = all_metadata, design = ~spike)
deseq_counts <- estimateSizeFactors(deseq_counts, type = "poscounts")
deseq_counts_vst <- varianceStabilizingTransformation(deseq_counts)
vst_trans_count_tab <- assay(deseq_counts_vst) # so this is a transformed table ?


#then astrobio mike uses this to make a euc distance

euc_dist = dist(vst_trans_count_tab)



#hierarchical clustering from astrobiomike w/ the euc_distances.... 
#this part seems useless to me
euc_clust <- hclust(euc_dist, method="ward.D2")

  # hclust objects like this can be plotted with the generic plot() function
plot(euc_clust) 
    # but i like to change them to dendrograms for two reasons:
      # 1) it's easier to color the dendrogram plot by groups
      # 2) if wanted you can rotate clusters with the rotate() 
      #    function of the dendextend package

euc_dend <- as.dendrogram(euc_clust, hang=0.1)
dend_cols <- as.character(all_metadata$spike[order.dendrogram(euc_dend)])
labels_colors(euc_dend) <- dend_cols

plot(euc_dend, ylab="VST Euc. dist.")


```

#Step 11: Make a phyloseq object W/ Variance Transformed seq tab from step 10 

```{r}
setwd("~/Desktop/Landfill sequencing data processing /PEAR_merged")

#rename the vst_trans_counts_tab.t. so that I know it has been changed
vst_trans_count_tab.t.n = vst_trans_count_tab.t
#add sample names to the vst_trans_count_tab.t.n.
rownames (vst_trans_count_tab.t.n) <- c("AB1","AB1.1","AB1.2","AB1.3","AB1.4","AB1.5", "AB2",   "AB2.1", "AB2.2", "AB2.3", "AB2.4" ,"AB2.5", "AB3"  , "AB3.1", "AB3.2", "AB3.3" ,"AB3.4","Fe1", "Fe1.1", "Fe1.2", "Fe1.3", "Fe1.4", "Fe1.5", "Fe2", "Fe2.1", "Fe2.2", "fe2", "Fe2.3", "Fe2.4", "Fe3", "Fe3.1", "Fe3.2", "Fe3.3", "Fe3.4", "Fe3.5", "NL1", "NL1.1", "NL1.2", "NL1.3", "NL2", "NL2.1", "NL2.2", "NL2.3", "NL3", "NL3.1", "NL3.2", "NL3.3", "NL4", "NL4.1", "NL4.2", "NL4.3", "NL4.4", "NL4.5", "NL5", "NL5.1", "NL5.2", "NL5.3", "NL5.4", "NL5.5", "NL6", "NL6.1", "NL6.2", "NL6.3", "NL6.4", "NL6.5","S1", "S1.1", "S1.2", "S1.3", "S1.4", "S1.5", "S2", "S2.1", "S2.2", "S2.3", "S2.4", "S2.5", "S3", "S3.1", "S3.2", "S3.3", "S3.4", "S3.5")

rownames(vst_trans_count_tab.t.n) #to check


#Now we need to make a new table that replaces the column names of vst_trans_count_tab.n with the column names of seqtab.nochim 
#first make a copy of the original vst_trans_count_tab.n to preserve the original. 
vst_tab_ps = vst_trans_count_tab.t.n
vst_tab_ps [1:10, 1:2] #check it
#now rewrite the column names to be the same as seqtab.nochim.n
colnames(vst_tab_ps) = colnames(nochim_seqtab.all)
#check it
vst_tab_ps [1:10, 1:2]



#Now we need to make a new table that replaces the column names of tax_tab with the column names of seqtab.nochim 
#first make a copy of the original tax_tab_t to preserve the original. 
taxtab_ps = t(tax_tab)
taxtab_ps #check it
dim(taxtab_ps)
dim (nochim_seqtab.all)
#now rewrite the column names to be the same as seqtab.nochim.n
colnames(taxtab_ps) = colnames(nochim_seqtab.all)


#Check to make sure these rownames are the same
identical(rownames(vst_tab_ps), rownames(all_metadata))
# should say TRUE
identical(colnames(vst_tab_ps), colnames(taxtab_ps))
# should say TRUE

#need to transpose the taxtab again 
taxtab_ps.t = t(taxtab_ps)

#Ok! Now make the object
seq_count_phy2 <- otu_table(vst_tab_ps, taxa_are_rows=FALSE)
tax_tab_phy2 <- tax_table(taxtab_ps.t)
Metadata_tab_phy2 <- sample_data(all_metadata)
my_vst_physeqobj = phyloseq(seq_count_phy2, Metadata_tab_phy2, tax_tab_phy2, phy_tree(all_fitGTR$tree))

my_vst_physeqobj_full_time = subset_samples(my_vst_physeqobj, bottle!="NL1" &  bottle!="NL2"& bottle!="NL3")
vst_all = my_vst_physeqobj_full_time #just renaming it to be a simplier name
 vst_all@sam_data[["bottle"]] #just to check 

```

#Step 11.1: Make a phyloseq object w/o Variance Transformed data 
(Continued from step 7, 8, 9 (no step 10))might have to re-do some of these steps (line 744 subsetting the phyloseq object)
```{r}

ps_all = phyloseq(otu_table(nochim_seqtab.all, taxa_are_rows = FALSE), sample_data(all_metadata), tax_table(taxtab.all), phy_tree(all_fitGTR$tree))

#don't need this one
#phyloseq object with all samples minus NL1-3 and AB1.3
#ps_all_1 = subset_samples(ps_all, sample!="NL1" & sample!="NL1.1" & sample!="NL1.2" & sample!="NL1.3"& sample!="NL2"& #sample!="NL2.1"& sample!="NL2.2"& sample!="NL2.3"& sample!="NL3"& sample!="NL3.1"& sample!="NL3.2"& sample!="NL3.3" #&sample!="AB1.3") #this is without NL1-3 and sample AB1-T4 because its low quality 


##phyloseq object with all live samples minus NL1-3
#ps_fulltime_live = subset_samples(ps_all, sample!="NL1" & sample!="NL1.1" & sample!="NL1.2" & sample!="NL1.3"& sample!="NL2"& #sample!="NL2.1"& sample!="NL2.2"& sample!="NL2.3"& sample!="NL3"& sample!="NL3.1"& sample!="NL3.2"& sample!="NL3.3")

#for some reason the "sample" column in all_metadata was removed, so I'm going to use a different variable to exclude those
ps_fulltime_live = subset_samples(ps_all, bottle!="NL1" &  bottle!="NL2"& bottle!="NL3")
all = ps_fulltime_live #just renaming it to be a simplier name
 all@sam_data[["bottle"]] #just to check 

#phyloseq object for killed
ps_killed = phyloseq(otu_table(nochim_seqtab_killed, taxa_are_rows = FALSE), sample_data(killed_metadata), tax_table(taxtab_killed), phy_tree(killed_fitGTR$tree))

#object for killed minus NK1-3
killed = subset_samples(ps_killed, sample!="NK1" & sample!="NK1.1" & sample!="NK1.2" & sample!="NK1.3"& sample!="NK2"& sample!="NK2.1"& sample!="NK2.2"& sample!="NK2.3"& sample!="NK3"& sample!="NK3.1"& sample!="NK3.2"& sample!="NK3.3" )



#each live experiment by itself

ps_NL = phyloseq(otu_table(nochim_seqtab.NL, taxa_are_rows = FALSE), sample_data(NL_metadata), tax_table(taxtab.NL))

ps_NL = phyloseq(otu_table(nochim_seqtab.NL, taxa_are_rows = FALSE), sample_data(NL_metadata), tax_table(taxtab.NL))

#this chunk removes NL 1-3 from the phyloseq object "new_ps_NL", these are the ones that run the full experiment length 
new_ps_NL = subset_samples(ps_NL, sample!="NL1" & sample!="NL1.1" & sample!="NL1.2" & sample!="NL1.3"& sample!="NL2"& sample!="NL2.1"& sample!="NL2.2"& sample!="NL2.3"& sample!="NL3"& sample!="NL3.1"& sample!="NL3.2"& sample!="NL3.3")
new_ps_NL

ps_S = phyloseq(otu_table(nochim_seqtab.S, taxa_are_rows = FALSE), sample_data(S_metadata), tax_table(taxtab.S))

ps_Fe = phyloseq(otu_table(nochim_seqtab.Fe, taxa_are_rows = FALSE), sample_data(Fe_metadata), tax_table(taxtab.Fe))


```


#save phyloseq objects onto the desktop
```{r}
setwd("~/Desktop/16s_script")
path_save = "~/Desktop/16s_script"

saveRDS(vst_all, file = "~/Desktop/16s_script/vst_all")
saveRDS(all, file = "~/Desktop/16s_script/all")
saveRDS(all, file = "~/Desktop/16s_script/killed" )
```



#reload the phyloseq objects 
```{r}

all = readRDS("~/Desktop/16s_script/all")
killed = readRDS("~/Desktop/16s_script/killed")
vst_all = readRDS("~/Desktop/16s_script/vst_all")


```



#                    PART II. Evaluating data in phylseq objects (5 steps) (Handley workflow)
    Step 1) Evaluate Amplicon Sequence Variants (ASV) summary statistics
    Step 2) Detect outlier samples
      Step 2.1) Remove outlier samples 
      
I haven't done steps 3-4 for the vst_all yet
    Step 3) Taxon cleaning
    Step 4) Prevalence assesment 
    Step 5) Prevalence filtering [try filtering at 1%]

#Step 1) Evaluate Amplicon Sequence Variants (ASV) summary statistics

#Step 2) Detect outlier samples
```{r}
#format a data table to combine summary data with sample variable data 
ss_all = sample_sums(all)
sd_all = as.data.frame(sample_data(all))
ss.dataf= merge(sd_all, data.frame("ASV" = ss_all), by="row.names")
ss.dataf

y= 1000 # set a thershold for th minimum number of acceptable reads, can start as a guess 
x= "sample_time" # the x-axis variable you want to examine 
label= "sample" 

#plot the data by the treatment variable 
ss_all_boxplot <- ggplot(ss.dataf, aes_string(x, y = "ASV", color = "spike")) + 
  geom_boxplot(outlier.colour="NA", position = position_dodge(width = 0.8)) +
  geom_jitter(size = 2, alpha = 0.6) +
  scale_y_log10() +
  facet_wrap(~spike) +
  geom_hline(yintercept = y, lty = 2) +
  geom_text(aes_string(label = label), size = 3, nudge_y = 0.05, nudge_x = 0.05)
ss_all_boxplot
#it looks like AB-T4 is an outlier in therms of ASVs 


#do the same for killed samples
#format a data table to combine summary data with sample variable data 
ss_killed = sample_sums(killed)
sd_killed = as.data.frame(sample_data(killed))
ss_killed_df=  merge(sd_killed, data.frame("ASV" = ss_killed), by="row.names")
ss_killed_df

y= 1000 # set a thershold for th minimum number of acceptable reads, can start as a guess 
x= "sample_time" # the x-axis variable you want to examine 
label= "sample" 

#plot the data by the treatment variable 
ss_killed_boxplot <- ggplot(ss_killed_df, aes_string(x, y = "ASV", color = "spike")) + 
  geom_boxplot(outlier.colour="NA", position = position_dodge(width = 0.8)) +
  geom_jitter(size = 2, alpha = 0.6) +
  scale_y_log10() +
  facet_wrap(~spike) +
  geom_hline(yintercept = y, lty = 2) +
  geom_text(aes_string(label = label), size = 3, nudge_y = 0.05, nudge_x = 0.05)
ss_killed_boxplot
#it looks like NK4-T1 is an outlier here 

```
#Step 2.1) Remove outlier samples 
Here it looks like NK5T1 and AB1T4 are outliers; maybe remove them? 
```{r}
nsamples(all)
nsamples(killed)

all <- all %>%
  subset_samples(
    bottle_number != "AB1t4")

all #check that the sample was removed


killed <- killed %>%
  subset_samples(
    bottle_number != "NK5t1")



```
#Step 3) Taxon cleaning
There may be some taxa that shouldn't be included in analysis, e.g. "Chloroplast / Cyanobacteria"
```{r}
get_taxa_unique(all1, "Kingdom")
get_taxa_unique(all1, "Phylum")
get_taxa_unique(killed1, "Kingdom")
get_taxa_unique(killed1, "Phylum")

#decided not to remove anything because this is trash. 

```
#Step 4) Prevalence assesment 
```{r}
rank_names(all1)
rank_names(killed1)

#create a table for the number of features for each phyla

table(tax_table(all1)[, "Phylum"], exclude = NULL)
table(tax_table(killed1)[, "Phylum"], exclude = NULL)

#compute prevalence of each feature + store as a data frome; defined as the number of samples in which taxa appears at least once
prev_all = apply(X = otu_table(all1), MARGIN = ifelse(taxa_are_rows(all1), yes = 1, no = 2), FUN = function(x){sum(x > 0)})
#add taxonomy and total read counts to this data frame
prev_all =data.frame(Prevalence = prev_all, TotalAbundance = taxa_sums(all1), tax_table(all1)) 
plyr::ddply(prev_all, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})


prev_killed = apply(X = otu_table(killed1), MARGIN = ifelse(taxa_are_rows(killed1), yes = 1, no = 2), FUN = function(x){sum(x > 0)})
prev_killed =data.frame(Prevalence = prev_killed, TotalAbundance = taxa_sums(killed1), tax_table(killed1)) 
plyr::ddply(prev_killed, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})


#Dashed horizontal line is drawn at 5% prevalence level
#explore the relationship of prevalence and total read count for each feature 
prev_all1 = subset(prev_all, Phylum %in% get_taxa_unique(all, "Phylum"))
ggplot(prev_all1, aes(TotalAbundance, Prevalence / nsamples(all),color=Family)) +
# Include a guess for parameter
geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) + geom_point(size = 2, alpha = 0.7) +
scale_x_log10() + xlab("Total Abundance") + ylab("Prevalence [Frac of Live Samples]") +
facet_wrap(~Phylum) + theme(legend.position="none")

prev_killed1 = subset(prev_killed, Phylum %in% get_taxa_unique(killed, "Phylum"))
ggplot(prev_killed1, aes(TotalAbundance, Prevalence / nsamples(killed),color=Family)) +
# Include a guess for parameter
geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) + geom_point(size = 2, alpha = 0.7) +
scale_x_log10() + xlab("Total Abundance") + ylab("Prevalence [Frac of Killed Samples]") +
facet_wrap(~Phylum) + theme(legend.position="none")




```
#Step 5) Prevalence filtering [try filtering at 1%]
```{r}
nsamples(all1)
prev_thres_all = 0.05 * nsamples(all1)
prev_thres_all
ntaxa(all1)

keepTaxa_all <- rownames(prev_all)[(prev_all$Prevalence >= prev_thres_all)]
ntaxa(all1)
all2 = prune_taxa(keepTaxa_all, all1)
ntaxa(all2) 


#threshold of 1% only removed 2 taxa, maybe i should set the threshold higher
#a 5% prevalence threshold took my taxa number from 1592 to 1566

#now do the same for the killed samples

prev_thres_k = 0.05*nsamples(killed1)
keeptaxa_k = rownames(prev_killed)[(prev_all$Prevalence >=prev_thres_k)]
ntaxa(killed1)
killed2=prune_taxa(keeptaxa_k, killed1)
ntaxa(killed2)
#a 5% prevalence threshold took my taxa from 1396 to 1390. 




```

### Repeating steps 1-2.1 for the vst_all phyloseq object
Step 1) Evaluate Amplicon Sequence Variants (ASV) summary statistics
```{r}
## Create a new data frame of the sorted row sums, a column of sorted values from 1 to the total number
#of individuals/counts for each ASV and a categorical variable stating these are all ASVs.
readsumsdf_vst  <- data.frame(nreads = sort(taxa_sums(vst_all), decreasing = TRUE), 
                         sorted = 1:ntaxa(vst_all),
                         type = "ASVs")

view(readsumsdf_vst)

sample_sum_df_vst <- data.frame(sum = sample_sums(vst_all))

# Make plots
# Generates a bar plot with # of reads (y-axis) for each taxa. Sorted from most to least abundant
# Generates a second bar plot with # of reads (y-axis) per sample. Sorted from most to least
p.reads_vst = ggplot(readsumsdf_vst, aes(x = sorted, y = nreads)) +
  geom_bar(stat = "identity") +
  ggtitle("ASV Assessment live samples") +
  scale_y_log10() +
  facet_wrap(~type, scales = "free") +
  ylab("# of Sequences")
p.reads_vst

# Histogram of the number of Samples (y-axis) at various read depths
p.reads.hist_vst <- ggplot(sample_sum_df_vst, aes(x = sum)) + 
  geom_histogram(color = "black", fill = "green", binwidth = 150) +
  ggtitle("Distribution of live sample sequencing depth") + 
  xlab("Read counts") +
  ylab("# of Samples")
p.reads.hist_vst

# Final plot, side-by-side
grid.arrange(p.reads, p.reads.hist, ncol = 2)

# Basic summary statistics
summary(sample_sums(vst_all))


```
Step 2) Detect outlier samples
```{r}
ss_all = sample_sums(all)
sd_all = as.data.frame(sample_data(all))
ss.dataf= merge(sd_all, data.frame("ASV" = ss_all), by="row.names")
ss.dataf

#format a data table to combine summary data with sample variable data 
ss_all_vst = sample_sums(vst_all)
sd_all_vst = as.data.frame(sample_data(vst_all))
ss.dataf_vst= merge(sd_all_vst, data.frame("ASV" = ss_all_vst), by="row.names")
ss.dataf_vst

view(ss.dataf_vst)

y= 1000 # set a thershold for th minimum number of acceptable reads, can start as a guess 
x= "sample_time" # the x-axis variable you want to examine 
label= "sample" 

#plot the data by the treatment variable 
ss_all_boxplot_vst <- ggplot(ss.dataf_vst, aes_string(x, y = "ASV", color = "spike")) + 
  geom_boxplot(outlier.colour="NA", position = position_dodge(width = 0.8)) +
  geom_jitter(size = 2, alpha = 0.6) +
  scale_y_log10() +
  facet_wrap(~spike) +
  geom_hline(yintercept = y, lty = 2) +
  geom_text(aes_string(label = label), size = 3, nudge_y = 0.05, nudge_x = 0.05)
ss_all_boxplot_vst

#in this case it looks like ABt4 is above the threshold.. how did that happen????


```
There are no outliers this time. I really need to figure out how this mixing model ACTUALLY WORKS.




#                          PART III: Let's make figures!   
    --> using the variance stabilizing transformation (vst) phyloseq object from here on out [except for the diversity plots e.g. shannon, simpson, etc.]
              
    0.0]Re-arranging samples 
    0.1]Assigning plot colors 
    0.2] Transform data into relative counts
A] Distance Measures; Ordination plots
B] Alpha diveristy
C] Taxa summaries
  C-1] Boxplots 
    0.4] make a dataframe from a phyloseq object.
  C-1-1] Arrange boxplots in a grid together
  C-2] Barcharts

-----
#0.0] Re-arrangle samples to put the control first
```{r}

#Take the phyloseq object apart and put it back together in the right order
sample_data(vst_all)$spike
control = subset_samples(vst_all, spike=="Control ")
antibiotics = subset_samples(vst_all, spike =="Antibiotics")
iron = subset_samples(vst_all, spike=="Fe(OH)3")
sulfur = subset_samples(vst_all, spike =="Na2SO4")

vst_all.ordered =merge_phyloseq(control, antibiotics, iron, sulfur)
vst_all.1 = vst_all #preserving the old phyloseq object
vst_all = vst_all.ordered

sample_data(vst_all.ordered)$spike

```

#0.1] Assigning plot colors
```{r}
exper_colors= c("blue", "gray50", "firebrick1", "gold")
exper_colors.ordered = c( "gray50", "blue", "firebrick1", "gold")

```

#0.2] Transform data into relative counts
```{r}
vst_all_rel_count = sweep(vst_trans_count_tab.abs, 1, rowSums(vst_trans_count_tab.abs), '/')

#OR 

vst_all_rel_1 = transform_sample_counts(vst_all, function(x) {x/sum(x)}) #from joey711/phyloseq issue #494

```

#A] Distance Measures; Ordination plots
```{r}

#NMDS + Bray curtis distances 

nmds_bray_vst_all = ordinate(vst_all, method="NMDS", distance = "bray")

nmds_bray_vst_all_plot = 
  plot_ordination(vst_all, nmds_bray_vst_all, color = "spike", shape ="sample_time", title = "Bray-Curtis NMDS")+ 
    scale_color_manual(values= exper_colors) + 
    geom_point(size=3.5)
nmds_bray_vst_all_plot #print the plot


#PCoA + Unifrac 

PCoA_wunifrac_vst_all = ordinate(vst_all, method="PCoA", distance = "wunifrac")

PCoA_wunifrac_vst_all_plot = 
  plot_ordination(vst_all , PCoA_wunifrac_vst_all, color = "spike", shape ="sample_time", title = "wUnifrac PCoA") +
  geom_point(size=3.5) +
  scale_color_manual(values= exper_colors)
PCoA_wunifrac_vst_all_plot

  
#PCoA + Unifrac with the ordered phyloseq object
PCoA_wunifrac_vst_all.ordered = ordinate(vst_all, method="PCoA", distance = "wunifrac")

PCoA_wunifrac_vst_all_plot.ordered = 
  plot_ordination(vst_all.ordered, PCoA_wunifrac_vst_all.ordered, color = "spike", shape ="sample_time", title = "wunifrac PCoA Ordered") +
    scale_color_manual(values= exper_colors) +
    geom_point(size=3.5)
PCoA_wunifrac_vst_all_plot.ordered



# so for some reason ordered in the phyloseq object with control first doesn't put the legend in order that the phyloseq object is, the only thing that happened is that the plot rotated 360 degrees.. I kinda like the ordered one better, it's easier to see somehow

```
#A-1] Create an ellipse around
This part doesn't work because there are too few points
```{r}

PCoA_wunifrac_vst_all_plot.ordered + scale_color_manual(values= exper_colors) + geom_point(size=3.5) +
stat_ellipse(type="norm", geom="polygon", alpha=1/10, aes (fills=sample_time))
# too few points
print(PCoA_wunifrac_vst_all.ordered +stat_ellipse())


```


#B] Alpha diversity 
Used to identify within individual taxa richness and evenness
The commonly used metrics/indices are Shannon, Inverse Simpson, Simpson, Gini, Observed and Chao1. These indices do not take into account the phylogeny of the taxa identified in sequencing. Phylogenetic diversity (Faith’s PD) uses phylogenetic distance to calculate the diversity of a given sample. (Sudarshan et al., 2020)


One has to consider the sequencing depth (how much of the taxa have been sampled) for each sample. If there is a large difference, then it is important to normalize the samples to equal sampling depth. First, we look at the sampling depth (no. of reads per sample).
```{r}
library(microbiome) # data analysis and visualisation
library(phyloseq) # also the basis of data object. Data analysis and visualisation
library(RColorBrewer) # nice color options
library(ggpubr) # publication quality figures, based on ggplot2
library(DT) # interactive tables in html and markdown
library(dplyr) # data handling  


print(vst_all)

summary(sample_sums(vst_all)) #number of reads per sample
#there isn't a large difference between the min and max 
#I'm going to look at the non vst phyloseq object 

summary(sample_sums(all))
#so obviously the vst trims down the number of reads significantly somehow. Again, I need to read the paper

#we can plot the rarefaction curve for the observed ASVs in the entire dataset, this is a way to check how the richness captured in the sequencing effort 

#otu_tab = t(abundances(vst_all)) 


otu_tab = t(abundances(all))  #t = transpose I think, not sure why we transposed it

#this step doesn't work with the vst_all, probably because of something abotu the way we transformed it


#plot of species/reads
#we're nto doing anything with this, but it's good to know
p = vegan::rarecurve(otu_tab, 
                      step = 50, label = FALSE, 
                      sample = min(rowSums(otu_tab), 
                                   col = "blue", cex = 0.6))

#the tutorial I'm using here used this plot to rarify their data, however, we're not doing this here because susan holmes would be mad.

```

Non-phylogenetic diversities (non vst)
```{r}
#make sure you have the most up-to-date version of the microbiome package; use #BiocManager::install("microbiome") #update all
#sometimes you have to unlaod and re-load the package for this step to work 
#let's calculate diversity
#this also doesn't work with vst_all
#turns out it's not suppose to work with the variance stabilizing transformation, so just use the original phyloseq object
div = alpha(all, index="all")  
data.table(div)

# get the metadata out as seprate object
meta <- meta(all)

# Add the rownames as a new colum for easy integration later.
meta$sam_name <- rownames(meta)

# Add the rownames to diversity table
div$sam_name <- rownames(div)

# merge these two data frames into one
div.df <- merge(div, meta, by = "sam_name")

# check the tables
colnames(div)


# Now use this data frame to plot 
p_div <- ggboxplot(div.df, 
               x = "spike", 
               y = "diversity_shannon",
              fill = "spike", 
              palette = "jco")

p_div <- p_div + rotate_x_text()

print(p_div)



#Seperate by both spike and time using metadata
p_div_1 <- ggboxplot(div.df, 
               x = "spike_time", 
               y = "diversity_shannon",
              fill = "sample_time")


p_div_1 <- p_div_1 + rotate_x_text()

print(p_div_1)

#I need to re-order the samples so that ABT4 is 




```


shannon phylogenetic diversity w/re-ordered samples (non vst)
```{r}
#Seperate by both spike and time using metadata

#order_of_samples = data.frame(name=c("ABT1", "ABT2", "ABT3", "ABT4" , "ABT5" ,"ABT6" ,"ABT1", "ABT2" ,
 #                                    "ABT3","ABT5", "ABT6" ,"ABT1", "ABT2", "ABT3",
  #                                   "ABT5", "ABT6" ,"FeT1","FeT2" ,"FeT3" ,"FeT4",
   #                                  "FeT5" ,"FeT6" ,"FeT4", "FeT1" ,"FeT2" ,"FeT3" ,
    #                               "FeT5" ,"FeT6", "FeT1" ,"FeT2" ,"FeT3", "FeT4" ,
     #                                "FeT5", "FeT6" ,"NLT1", "NLT2", "NLT3", "NLT4",
      #                               "NLT5", "NLT6" ,"NLT1", "NLT2" ,"NLT3" ,"NLT4",
       #                              "NLT5" ,"NLT6", "NLT1", "NLT2" ,"NLT3", "NLT4" ,
        #                             "NLT5" ,"NLT6" ,"ST1" , "ST2" , "ST3" , "ST4",
         #                            "ST5" , "ST6" , "ST1" , "ST2" , "ST3" , "ST4" , 
          #                           "ST5",  "ST6",  "ST1" , "ST2",  "ST3",  "ST4", 
           #                          "ST5" , "ST6"))

#ignore, but I'm keeping the order just in case!! 

as.factor(div.df$spike_time)

div.df$spike_time = factor(div.df$spike_time, levels = c("ABT1", "ABT2", "ABT3", "ABT4" , "ABT5" ,"ABT6", "FeT1","FeT2" ,"FeT3" ,"FeT4", "FeT5" ,"FeT6", "NLT1", "NLT2", "NLT3", "NLT4", "NLT5", "NLT6", "ST1" , "ST2",  "ST3",  "ST4", "ST5" , "ST6"))


p_div_1 <- ggboxplot(div.df, 
               x = "spike_time", 
               y = "diversity_shannon",
              fill = "sample_time")


p_div_1 <- p_div_1 + rotate_x_text()

print(p_div_1)

#THAT WORKED, IT'S ORDERED NOW!! ONLY TOOK 1 HOUR TO FIGURE OUT. 


```



simpson boxplots (non vst)
```{r}
#spike time
p_even <- ggboxplot(div.df, 
               x = "spike_time", 
               y = "evenness_simpson",
              fill = "sample_time")


p_even <- p_even + rotate_x_text()

print(p_even)

```



Shannon and Simpson (vst)
```{r}


Shannon_plot = plot_richness(vst_all.ordered, x="spike_time", measures=c("Shannon"), color="spike")
Shannon_plot + scale_color_manual(values= exper_colors) 





Simpson_plot = plot_richness(vst_all, x="spike_time", measures=c("Simpson"), color="spike")
Simpson_plot  + scale_color_manual(values= exper_colors)
```


Phylogenetic diversity using the Picante packate 
https://mibwurrepo.github.io/Microbial-bioinformatics-introductory-course-Material-2018/alpha-diversities.html#phylogenetic-diversity
all of my diveristy measures were the same, I give up on this for now
```{r, include=FALSE}

library(picante)

vst_all_asvtab = as.data.frame(vst_all@otu_table) #otu tab as a data frame
view(vst_all_asvtab)

vst_all_tree = vst_all@phy_tree #seperating the phylogenetic tree 


# We first need to check if the tree is rooted or not 

vst_all@phy_tree #unrooted tree 

#df.pd <- pd(t(vst_all_asvtab), vst_all_tree, include.root=T) # t(ou_table) transposes the table for use in picante and the tre file comes from the first code chunck we used to read tree file 

#need to root the tree, using joey711/phyloseq comment from Feb 28, 2018, issue #597 
#https://github.com/joey711/phyloseq/issues/597

pick_new_outgroup <- function(tree.unrooted){
    require("magrittr")
    require("data.table")
    require("ape") # ape::Ntip
    # tablify parts of tree that we need.
    treeDT <- 
      cbind(
        data.table(tree.unrooted$edge),
        data.table(length = tree.unrooted$edge.length)
      )[1:Ntip(tree.unrooted)] %>% 
      cbind(data.table(id = tree.unrooted$tip.label))
    # Take the longest terminal branch as outgroup
    new.outgroup <- treeDT[which.max(length)]$id
    return(new.outgroup)
}


#It returns the string of the tip-label for the new proposed outgroup. It is then up to the user/process to take that result and do the next rooting step. This is a bit easier to test and maintain than also doing the rooting and opaquely returning the tree.

new.outgroup = pick_new_outgroup(vst_all_tree)
rootedTree= ape::root(vst_all_tree, outgroup=new.outgroup, resolve.root=TRUE)

rootedTree_vst_all = rootedTree
rootedTree_vst_all


#try again to the the phylogentic diversity 

#df.pd <- pd(t(vst_all_asvtab), rootedTree,include.root=T)
## Error in UseMethod("is.rooted") : no applicable method for 'is.rooted' applied to an object of class "NULL"

#okay I'll try to make my phyloseq object again with the rooted tree and then repeat 


my_vst_physeqobj_rooted = phyloseq(seq_count_phy2, Metadata_tab_phy2, tax_tab_phy2, phy_tree(rootedTree))

my_vst_physeqobj_full_time_rooted = subset_samples(my_vst_physeqobj_rooted, bottle!="NL1" &  bottle!="NL2"& bottle!="NL3")

vst_all_rooted = my_vst_physeqobj_full_time_rooted #just renaming it to be a simplier name
 vst_all@sam_data[["bottle"]] #just to check 
 
vst_all_rooted_asvtab = as.data.frame(vst_all_rooted@otu_table)
vst_all_rooted_tree = vst_all_rooted@phy_tree


#df.pd = pd(vst_all_rooted_asvtab, vst_all_rooted_tree) #all i needed to do was NOT transpose the tree like they suggested. ugh.
#datatable(df.pd) #why is pd all the same? 

#let's try transposing first and then doing the df.pd
vst_all_rooted_asvtab.t = t(vst_all_rooted_asvtab)

df.pd = pd(vst_all_rooted_asvtab.t, vst_all_rooted_tree)

vst_all_rooted_asvtab.t


# get the metadata out as seprate object
vst_meta <- meta(vst_all_rooted)


vst_meta$Phylogenetic_Diversity = df.pd$PD #added the phylogenetic diversity results to the vst_metadata

view(vst_meta)





```






#0.3]taxa glom
```{r}
#summes all ASVs that were in the same level

#Glom at famiily level 
vst_all_rel_glom = tax_glom(vst_all_rel_1, taxrank = "Family")
vst_all_rel_glom_family = vst_all_rel_glom

#Glom at phylum level 
vst_all_rel_glom_phylum = tax_glom(vst_all_rel_1, taxrank= "Phylum")



```


#C-1] Boxplots 
```{r}

#Phylum level box plot 

vst_all_rel_glom2 = tax_glom(vst_all_rel_1, taxrank = "Phylum")

dat_vst_all_rel2 = data.table(psmelt(vst_all_rel_glom2))
dat_vst_all_rel2$Phylum <- as.character(dat_vst_all_rel2$Phylum)
dat_vst_all_rel2$Order = as.character(dat_vst_all_rel2$Order)
dat_vst_all_rel2$Family = as.character(dat_vst_all_rel2$Family)

# group dataframe by Family, calculate median rel. abundance
dat_vst_all_rel2[, median := median(Abundance, na.rm = TRUE), 
    by = "Phylum"]
# Change name to remainder of Family less than .1%
dat_vst_all_rel2[(median <= 0.001), Phylum := " Other < .1%"]


boxplot_Phylum = ggplot(dat_vst_all_rel2[Abundance > 0],
       aes(x=Phylum,
           y=Abundance)) + 
  geom_boxplot() + 
  coord_flip() +
  scale_y_log10()+ labs(title="Phylum")
boxplot_Phylum


#Family level box plot

vst_all_rel_glom = tax_glom(vst_all_rel_1, taxrank = "Family")

dat_vst_all_rel = data.table(psmelt(vst_all_rel_glom))
dat_vst_all_rel$Phylum <- as.character(dat_vst_all_rel$Phylum)
dat_vst_all_rel$Order = as.character(dat_vst_all_rel$Order)
dat_vst_all_rel$Family = as.character(dat_vst_all_rel$Family)

# group dataframe by Family, calculate median rel. abundance
dat_vst_all_rel[, median := median(Abundance, na.rm = TRUE), 
    by = "Family"]
# Change name to remainder of Family less than 5%
dat_vst_all_rel[(median <= 0.01), Family := " Other < 1%"]


boxplot_Family = ggplot(dat_vst_all_rel[Abundance > 0],
       aes(x=Family,
           y=Abundance)) + 
  geom_boxplot() + 
  coord_flip() +
  scale_y_log10()+ labs(title="Family")
boxplot_Family



#genus level boxplot
vst_all_rel_glom_genus = tax_glom(vst_all_rel_1, taxrank = "Genus")

dat_vst_all_rel_genus = data.table(psmelt(vst_all_rel_glom_genus)) #make a data table with the agg taxa at genus level 
dat_vst_all_rel_genus$Phylum <- as.character(dat_vst_all_rel_genus$Phylum)
dat_vst_all_rel_genus$Order = as.character(dat_vst_all_rel_genus$Order)
dat_vst_all_rel_genus$Family = as.character(dat_vst_all_rel_genus$Family)
dat_vst_all_rel_genus$Genus = as.character(dat_vst_all_rel_genus$Genus)



# group dataframe by Genus, calculate median rel. abundance
dat_vst_all_rel_genus[, median := median(Abundance, na.rm = TRUE), 
    by = "Genus"]
# Change name to remainder of Family less than 1%
dat_vst_all_rel_genus[(median <= 0.01), Genus := " Other < 1%"]


boxplot_genus = ggplot(dat_vst_all_rel_genus[Abundance > 0],
       aes(x=Genus,
           y=Abundance)) + 
  geom_boxplot() + 
  coord_flip() +
  scale_y_log10()+ labs(title="Genus")
boxplot_genus


```

#C-1-1] Arrange boxplots in a grid together
```{r}
ggarrange(boxplot_Phylum, boxplot_Family, nrow=1)
boxplot_genus

```

#0.4] Make a dataframe from a phyloseq object
```{r}

dat_family = data.table(psmelt(vst_all_rel_glom_family))
# convert Phylum to a character vector from a factor because R
dat_family$Phylum <- as.character(dat$Phylum)
dat_family$Order = as.character(dat$Order)
dat_family$Family = as.character(dat$Family)

# group dataframe by Family, calculate median rel. abundance
dat_family[, median := median(Abundance, na.rm = TRUE), 
    by = "Family"]
# Change name to remainder of Family less than 1%
dat_family[(median <= 0.01), Family := " Other < 1%"]
```


#C-2] Barcharts 
phylum
```{r}

# Create a data table for ggplot
vst_all_phylum <- vst_all %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance (or use ps0.ra)
  psmelt() %>%                                         # Melt to long format for easy ggploting
  filter(Abundance > 0.01)                             # Filter out low abundance taxa

# Plot - Phylum
p.ra.phylum <- ggplot(vst_all_phylum, aes(x = spike_time, y = Abundance, fill = Phylum)) + 
  geom_bar(stat = "identity", width = 1) +
  facet_wrap(spike~spike_time, scales = "free_x", nrow = 4, ncol = 7) +
  theme(axis.text.x = element_blank()) +
  theme(axis.title.x = element_blank()) +
  labs(title = "Abundant Phylum (> 1%)")
p.ra.phylum


# Create a data table for ggplot
vst_all_phylum <- vst_all %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance (or use ps0.ra)
  psmelt() %>%                                         # Melt to long format for easy ggploting
  filter(Abundance > 0.01)                             # Filter out low abundance taxa

# Plot - Phylum
p.ra.phylum <- ggplot(vst_all_phylum, aes(x = sample_time, y = Abundance, fill = Phylum)) + 
  geom_bar(stat = "identity", width = 1) +
  labs(title = "Abundant Phylum (> 1%)")
p.ra.phylum

mean(vst_all_phylum$Abundance~sample_time)

abundance_by_time = vst_all_phylum %>% group_by(sample_time)
levels(abundance_by_time$sample_time)
as.factor(abundance_by_time$sample_time)

rel_abundance_T1 = vst_all_phylum %>% subset(sample_time == "T1")




```
Genus
```{r}
# Create a data table for ggplot
vst_all_genus <- vst_all %>%
  tax_glom(taxrank = "Genus") %>%                     # agglomerate at phylum level
  psmelt()                                        # Melt to long format for easy ggploting
     
  
vst_all_genus_rel <- vst_all %>%
  tax_glom(taxrank="Genus") %>%
  transform_sample_counts(function(x) {x/sum(x)}) %>% # Transform to rel
  psmelt()
                                        
  
vst_all_genus$relative_abundance = vst_all_genus_rel$Abundance

vst_all_genus_filt = vst_all_genus %>%filter(relative_abundance>0.01)
vst_all_genus_filt_5 = vst_all_genus %>%filter(relative_abundance>0.05)



# Plot - Genus
p.ra.genus <- ggplot(vst_all_genus_filt_5, aes(x = sample_time, y = Abundance, fill = Genus)) + 
  geom_bar(aes(color=Genus, fill=Genus), stat = "identity", width = 1) +
  labs(title = "Abundant Genus ( >5 %)", x = "Sample Time")
p.ra.genus


# Plot - Genus
p.ra.genus_2 <- ggplot(vst_all_genus_filt_5, aes(x = spike, y = Abundance, fill = Genus)) + 
  geom_bar(aes(color=Genus, fill=Genus), stat = "identity", width = 1) +
  labs(title = "Abundant Genus (>5 %)", x="Spike")
p.ra.genus_2

#plot genus 

p.ra.genus_3 <- ggplot(vst_all_genus_filt, aes(x = bottle , y = relative_abundance, fill = Genus)) +
  geom_bar(aes(color=Genus, fill=Genus), stat = "identity", width = 1) +
  labs(title = "Abundant Genus (>1 %)", x="Spike")+
  facet_wrap(sample_time~spike, scales="free_x", nrow=4)
p.ra.genus_3


```



#              Part IIII: Significance Testing and Other statistics 

0.I] subsetting data in phyloseq object  
I. ADONIS (Handley workflow)
II. Differential Abundance testing (DESEQ2) # Hasn't quite worked yet Need to read the DESEQ2 paper and workflow maybe?
III. Random Forest Modelling

Links to other methods to detect differentially abundant taxa:

Random Forest in R example: https://rpubs.com/michberr/randomforestmicrobe
LEfSe: https://bitbucket.org/biobakery/biobakery/wiki/lefse
ANCOM: https://www.ncbi.nlm.nih.gov/pubmed/26028277


----
#0.I] Subsetting data
```{r}
#Make a phyloseq object with just the post-spike time points
sample_data(vst_all)$sample_time #check what the levels look like
postspike_vst_all = subset_samples(vst_all, sample_time!="T1" & sample_time!="T2" & sample_time!="T3" & sample_time!="T4") #exclude anything before the spikes were added
sample_data(postspike_vst_all)$sample_time #check again 


#Make a phyloseq object with just the pre-spike time points
sample_data(vst_all)$sample_time #check what the levels look like
prespike_vst_all = subset_samples(vst_all, sample_time!="T5" & sample_time!="T6")
sample_data(prespike_vst_all)$sample_time #check again


#Make objects with just control and treament group
sample_data(vst_all)$spike

control_antibiotics = subset_samples(vst_all, spike!="Fe(OH)3" & spike!="Na2SO4")
sample_data(control_antibiotics)$spike

control_iron = subset_samples(vst_all, spike!="Antibiotics" & spike!="Na2SO4")
sample_data(control_iron)$spike

control_sulfate = subset_samples(vst_all, spike!="Fe(OH)3" & spike!="Antibiotics")
sample_data(control_sulfate)$spike

```


#I. Group Signifcance Testing with ADONIS (from handely workflow) (done)
another way:
https://mibwurrepo.github.io/Microbial-bioinformatics-introductory-course-Material-2018/beta-diversity-metrics.html#phylogenetic-beta-diversity-metrics


paper source:
https://onlinelibrary.wiley.com/doi/10.1002/9781118445112.stat07841

```{r}
#Set a random seed so that exact results can be reproduced
#I honestly don't know what that means. 
set.seed(10000)


#function to run ADONIS test on a phyloseq object and a variable from metadata 
doadonis <- function(physeq, category) {
  bdist <- phyloseq::distance(physeq, "wunifrac")
  col <- as(sample_data(physeq), "data.frame")[ ,category]
  # Adonis test
  adonis.bdist <- adonis(bdist ~ col, permutations = 10000) 
  #the great number of permuations gives greater precision, but also takes longer to run, although this didn't take that long
  print("Adonis results:")
  print(adonis.bdist)}


all_timepoints_adonis = doadonis(vst_all, "spike") #Results are significant, though I'm not sure which groups are different from each other. 

all_timepoints_adonis$aov.tab$'Pr(>F)'[1] #this gives you the p value of the adonis test 
#same thing but in another way; double bracket notation; 
all_timepoints_adonis[["aov.tab"]][["Pr(>F)"]][1]


#control v. antibiotics
control_antibiotics_adonis = doadonis(control_antibiotics, "spike") #sig

#control v. iron 
control_iron_adonis = doadonis(control_iron, "spike") #sig

#control v. sulfate
control_sulfate_adonis = doadonis(control_sulfate, "spike") #sig


##adonis of samples only pre-spike 
pre_spike_adonis = doadonis(prespike_vst_all, "spike") #significant

##adonis of samples only post-spike 
post_spike_adonis = doadonis(postspike_vst_all, "spike") #significant 


#it seems like the experiments were different even before the spikes


```

#I-1. Checking the homogeneity condition? (not yet)
https://mibwurrepo.github.io/Microbial-bioinformatics-introductory-course-Material-2018/beta-diversity-metrics.html#permanova

```{r}




```




#II. Differential Abundance Testing [[This didn't work yet]]
DESEQ2
Since the example data set collected time-series data we can take advantage of the likelihood ration test (LRT) model of DESeq2. The LRT will identify ASV differentially abundant between groups at one or more time point.
Determine all taxa differentially abundant between treatment groups at at least one time point
```{r}

# Test for taxa which at one or more time points after time 0 showed a treatment-specific effect
# Convert phyloseq object to DESeq2 table
ds_control_antibiotics_LRT <- phyloseq_to_deseq2(control_antibiotics, ~spike + sample_time + spike:sample_time)

# Run DESeq2
dds_control_antibiotics_LRT <- DESeq(ds_control_antibiotics_LRT, test="LRT", reduced = ~spike + sample_time)



# Tabulate results
res.dds.vehicle_metro.LRT <- results(dds.vehicle_metro.LRT)
res.dds.vehicle_metro.LRT$symbol <- mcols(dds.vehicle_metro.LRT)$symbol
summary(res.dds.vehicle_metro.LRT)
mcols(res.dds.vehicle_metro.LRT)
write.table(res.dds.vehicle_metro.LRT, file = "../results/deseq_vehicle_v_metro_LRT.txt", sep = "\t")

nrow(res.dds.vehicle_metro.LRT)
df.res <- as.data.frame(res.dds.vehicle_metro.LRT[ which(res.dds.vehicle_metro.LRT$padj < 0.05), ])
nrow(df.res)
df.res <- rownames_to_column(df.res, var = "ASV")
write.table(df.res, file = "../results/df_deseq2_results.txt", sep = "\t")

# Create appropriately formatted taxa table
# RDP
tax.table <- as.tibble(as.data.frame(tax_table(ps2)))
tax.table <- rownames_to_column(tax.table, var = "ASV")
df.rdp <- left_join(df.res, tax.table, by = "ASV")
colnames(df.rdp)

ggplot(df.rdp, aes(x = Phylum, y = log2FoldChange, color = Family)) +
  geom_jitter(size = 3, alpha = 0.7, width = 0.1) +
  geom_hline(yintercept = 0, lty = 2) +
  ylim(-35, 35)




dds = DEseq(dds)





```





#III. Random Forest Modelling
https://rpubs.com/michberr/randomforestmicrobe
```{r}

#need to do some basic reformatting and filtering 
colnames(tax_table(vst_all))

# Filter out, chloroplasts and mitochondria k
vst_all %>%
  subset_taxa( Family != "mitochondria" & 
              Class != "Chloroplast") -> vst_all 

vst_all





```



#Core Microbiotaanalysis
https://mibwurrepo.github.io/Microbial-bioinformatics-introductory-course-Material-2018/core-microbiota.html

```{r}

vst_all_rel_1 = transform_sample_counts(vst_all, function(x) {x/sum(x)}) #from joey711/phyloseq issue #494

print(vst_all_rel_1)

vst_all_rel_pruned = prune_taxa(taxa_sums(vst_all_rel_1)>0, vst_all_rel_1)

print(vst_all_pruned) #nothing changed so not sure why we did this step 

core.taxa.standard = core_members(vst_all_rel_pruned, detection = 0.001)

print(core.taxa.standard)

taxonomy_rel = as.data.frame(tax_table(vst_all_rel_1))

#subset taxomony to include only core OTUs

core_taxa_id = subset(taxonomy_rel, rownames(taxonomy_rel) %in% core.taxa.standard) #these are the core taxa that we've identified


#next we'll do the sum of the abundances of the core members in each sample

core.abundance <- sample_sums(core(vst_all_rel_1, detection = 0.001, prevalence = 50/100))


new = prune_taxa(core_taxa_id, vst_all_rel_1)



```



#Advanced Models for differential abudnance
https://mibwurrepo.github.io/Microbial-bioinformatics-introductory-course-Material-2018/advanced-models-for-differential-abundance.html

```{r}

```



#Trying to create a count tab with meaningful rownames
incomplete, ignore
```{r}

taxonomy = as.data.frame(tax_table(vst_all)) #extracting the taxatable from vst_all phyloseq object 

counts_all = as.data.frame(otu_table(vst_all)) #extracting the otu table 
counts_all.t = t(counts_all) #transposing the otutable

taxonomy$taxalabel = paste(new_taxanames$Family, new_taxanames$Genus) #adding a column with the Family and Genus name in one label 
replace(taxonomy$taxalabel, taxonomy$taxalabel=="NA NA", "Other") #placing the "NA NA" data with "other"

taxonomy$taxalabel2 = paste(new_taxanames$Order, new_taxanames$Family) 

replace(taxonomy$taxalabel2, taxonomy$taxalabel2=="NA NA", "Other")

identical(rownames(counts_all.t), rownames(taxonomy))#are the rownames identifical?
#TRUE

rownames(taxonomy) = taxonomy$taxalabel2

summary(vst_all)




```




